{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filenames):\n",
    "    arrays = [0,0,0]\n",
    "    for i in xrange(len(filenames)):\n",
    "        arrays[i] = pd.read_csv(filenames[i])\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def yes_1(x):\n",
    "    return x.replace('yes', 1).replace('no',-1).replace(\"excellent\",2).replace(\"good\",1).replace(\n",
    "        \"satisfactory\",0).replace(\"poor\",-1).replace(\"no data\",0).replace(\"Investment\", 1).replace(\"OwnerOccupier\",-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_impute(data):\n",
    "    data[\"life_sq\"]=data[\"life_sq\"]/data[\"full_sq\"]\n",
    "    data[\"max_floor\"]=data[\"max_floor\"]/data[\"floor\"]\n",
    "    data[\"num_room\"]=data[\"num_room\"]/data[\"full_sq\"]\n",
    "    data['green_part_2000']=data['green_part_2000'].fillna(data['green_part_1500'])\n",
    "    data['prom_part_5000']=data['prom_part_5000'].fillna(data['prom_part_2000'])\n",
    "    data=data.apply(yes_1)\n",
    "    medImputer=Imputer(strategy='median')\n",
    "    mfImputer=Imputer(strategy='most_frequent')\n",
    "    data[[\"material\",\"build_year\",\"state\",\"product_type\"]]=mfImputer.fit_transform(data[[\"material\",\"build_year\",\"state\",\"product_type\"]])\n",
    "    data[[\"life_sq\",\"max_floor\",\"num_room\",\"kitch_sq\"]]=medImputer.fit_transform(data[[\"life_sq\",\"max_floor\",\"num_room\",\"kitch_sq\"]])\n",
    "    data[\"build_year\"]=2017-data[\"build_year\"]\n",
    "    return data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_transport(data):\n",
    "    data[\"metro_km_walk\"]=data[\"metro_km_walk\"].fillna(data[\"metro_km_avto\"])\n",
    "    data[\"metro_min_walk\"]=data[\"metro_min_walk\"].fillna(data[\"metro_km_avto\"]*12)\n",
    "    data[\"railroad_station_walk_km\"]=data[\"railroad_station_walk_km\"].fillna(data[\"railroad_station_avto_km\"])\n",
    "    data[\"railroad_station_walk_min\"]=data[\"railroad_station_walk_min\"].fillna(data[\"railroad_station_avto_km\"]*12)\n",
    "    data[\"ID_railroad_station_walk\"]=data[\"ID_railroad_station_walk\"].fillna(data[\"ID_railroad_station_avto\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_cafe(data):\n",
    "    st0=['cafe_sum_5000_min_price_avg','cafe_sum_5000_max_price_avg','cafe_avg_price_5000']\n",
    "    l=['5000','3000','2000','1500','1000','500']\n",
    "    medImputer=Imputer(strategy='median')\n",
    "    data[st0]=medImputer.fit_transform(data[st0])\n",
    "    for i in xrange(len(l)-1):\n",
    "        st1=[st.replace('5000',str(l[i+1])) for st in st0]\n",
    "        st2=[st.replace('5000',str(l[i])) for st in st0]\n",
    "        for k in xrange(len(st1)):\n",
    "            data[st1[k]]=data[st1[k]].fillna(data[st2[k]])\n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rm_col(data, thrs):\n",
    "    k=len(data)-data.count()\n",
    "    return list(k[k<thrs].index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "def data_transform(data, macro, div):\n",
    "    data[\"timestamp\"]=pd.to_datetime(data[\"timestamp\"],format='%Y-%m-%d')\n",
    "    macro[\"timestamp\"]=pd.to_datetime(macro[\"timestamp\"],format='%Y-%m-%d')\n",
    "    data = data.merge(macro, left_on='timestamp', right_on='timestamp', how='left')   \n",
    "    data=my_impute(data)\n",
    "    data=data.ix[data['floor']>0]\n",
    "    data=data.ix[data['full_sq']>0]\n",
    "    data=my_transport(data)\n",
    "    data=my_cafe(data)\n",
    "    data1 = data[data[div[0]].isin(div[1])]\n",
    "    data2 = data[-data[div[0]].isin(div[1])]\n",
    "    data1=data1[rm_col(data1,650)]\n",
    "    data2=data2[rm_col(data2,1800)]\n",
    "    return data1, data2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printna(data):\n",
    "    k=len(data)-data.count()\n",
    "    print k[k>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def er10(data):\n",
    "    err = (data.full_sq>data.life_sq*10)&(data.life_sq>2)\n",
    "    data.ix[err,'full_sq'] = data.ix[err,'full_sq']/10\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def excl_cheat(data, quan = .05):\n",
    "    data = data[data[\"price_doc\"]>2000000]\n",
    "    df = data.groupby(\"sub_area\")\n",
    "    result = pd.DataFrame()\n",
    "    for name, group in df:\n",
    "        q = group.pr_RUB_SQM.quantile(quan)\n",
    "        group = group[group.pr_RUB_SQM > q]\n",
    "        result = result.append(group)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = [\"./data/macro.csv\",\"./data/train.csv/train.csv\",\"./data/test.csv/test.csv\"]\n",
    "[macro, data, data_test] = read_data(filenames)\n",
    "data=er10(er10(data))\n",
    "data[\"pr_RUB_SQM\"] = data[\"price_doc\"]/data[\"full_sq\"]\n",
    "#data = excl_cheat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test[\"full_sq\"]=data_test[\"full_sq\"].fillna(data_test[\"life_sq\"])\n",
    "data_test.ix[data_test[\"full_sq\"]==0,'full_sq'] = data_test.ix[data_test[\"full_sq\"]==0,'life_sq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "div=['sub_area',data[data.preschool_quota.isnull()]['sub_area'].value_counts().index.tolist()]\n",
    "[data1, data2]=data_transform(data, macro, div)\n",
    "[data_test1, data_test2]=data_transform(data_test, macro, div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test2=data_test2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "printna(data_test1)\n",
    "printna(data_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def larger_model(dim, reg=0.01):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim, input_dim=dim, kernel_initializer='random_normal', activation='relu', \n",
    "                    kernel_regularizer=regularizers.l2(reg),activity_regularizer=regularizers.l1(reg)))\n",
    "    model.add(Dense(148, kernel_initializer='random_normal', activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(reg),activity_regularizer=regularizers.l1(reg)))\n",
    "    model.add(Dense(20, kernel_initializer='random_normal', activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(reg),activity_regularizer=regularizers.l1(reg)))\n",
    "    model.add(Dense(1, kernel_initializer='random_normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NN1=load_model('nn1.h5')\n",
    "#NN2=load_model('nn2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex=[\"id\",\"timestamp\",\"sub_area\", \"ID_metro\", \"ID_railroad_station_walk\", \"ID_railroad_station_avto\", \n",
    "    \"ID_big_road1\", \"ID_big_road2\", \"ID_railroad_terminal\", \"ID_bus_terminal\", 'hospital_beds_raion']\n",
    "v1=[x for x in list(data_test1.columns.values) if x not in ex]\n",
    "v=[x for x in list(data_test2.columns.values) if x not in ex]\n",
    "Y2, X2, X2_test = map(np.array, [data2[\"price_doc\"], data2[v1], data_test2[v1]])\n",
    "batch_size2=X2.shape[0]/50 \n",
    "#x_train, x_test = labels(x_train), labels(x_test||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN2 = larger_model(dim=X2.shape[1], reg=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas SGEMM launch failed : a.shape=(473, 296), b.shape=(296, 296), m=473, n=296, k=296\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dense_1_input_0/_9, dense_1/kernel/read)]]\n\t [[Node: add_5/_37 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_567_add_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'dense_1/MatMul', defined at:\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-a63297e679c9>\", line 1, in <module>\n    NN2 = larger_model(dim=X2.shape[1], reg=0.01)\n  File \"<ipython-input-20-ba4441c0640c>\", line 5, in larger_model\n    kernel_regularizer=regularizers.l2(reg),activity_regularizer=regularizers.l1(reg)))\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/models.py\", line 433, in add\n    layer(x)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/engine/topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/layers/core.py\", line 840, in call\n    output = K.dot(inputs, self.kernel)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 936, in dot\n    out = tf.matmul(x, y)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1765, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1454, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Blas SGEMM launch failed : a.shape=(473, 296), b.shape=(296, 296), m=473, n=296, k=296\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dense_1_input_0/_9, dense_1/kernel/read)]]\n\t [[Node: add_5/_37 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_567_add_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-fd412fc33649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNN2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mNN2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2229\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1035\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas SGEMM launch failed : a.shape=(473, 296), b.shape=(296, 296), m=473, n=296, k=296\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dense_1_input_0/_9, dense_1/kernel/read)]]\n\t [[Node: add_5/_37 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_567_add_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'dense_1/MatMul', defined at:\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-a63297e679c9>\", line 1, in <module>\n    NN2 = larger_model(dim=X2.shape[1], reg=0.01)\n  File \"<ipython-input-20-ba4441c0640c>\", line 5, in larger_model\n    kernel_regularizer=regularizers.l2(reg),activity_regularizer=regularizers.l1(reg)))\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/models.py\", line 433, in add\n    layer(x)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/engine/topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/layers/core.py\", line 840, in call\n    output = K.dot(inputs, self.kernel)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 936, in dot\n    out = tf.matmul(x, y)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1765, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1454, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Blas SGEMM launch failed : a.shape=(473, 296), b.shape=(296, 296), m=473, n=296, k=296\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dense_1_input_0/_9, dense_1/kernel/read)]]\n\t [[Node: add_5/_37 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_567_add_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "NN2.fit(X2,Y2, batch_size=batch_size2, epochs=200, verbose=1)\n",
    "NN2.fit(X2,Y2, batch_size=batch_size2, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.28503122598\n"
     ]
    }
   ],
   "source": [
    "print r2_score(NN2.predict(X2),Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NN2.save('nn2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y1, X1, X1_test = map(np.array, [data1[\"price_doc\"], data1[v1], data_test1[v1]])\n",
    "#x_train, x_test = labels(x_train), labels(x_test||\n",
    "batch_size1=X1.shape[0]/50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN1 = larger_model(dim=X1.shape[1], reg=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6602/6602 [==============================] - 0s - loss: 352980461476054.0625     \n",
      "Epoch 2/200\n",
      "6602/6602 [==============================] - 0s - loss: 67254951981881.1562     \n",
      "Epoch 3/200\n",
      "6602/6602 [==============================] - 0s - loss: 33718847109470.5352     \n",
      "Epoch 4/200\n",
      "6602/6602 [==============================] - 0s - loss: 30657066370910.0703     \n",
      "Epoch 5/200\n",
      "6602/6602 [==============================] - 0s - loss: 30273927765035.7422     \n",
      "Epoch 6/200\n",
      "6602/6602 [==============================] - 0s - loss: 30234543037012.2188     \n",
      "Epoch 7/200\n",
      "6602/6602 [==============================] - 0s - loss: 30225847383342.4531     \n",
      "Epoch 8/200\n",
      "6602/6602 [==============================] - 0s - loss: 30223609316831.8945     \n",
      "Epoch 9/200\n",
      "6602/6602 [==============================] - 0s - loss: 30222367389897.6367     \n",
      "Epoch 10/200\n",
      "6602/6602 [==============================] - 0s - loss: 30221424398925.3984     \n",
      "Epoch 11/200\n",
      "6602/6602 [==============================] - 0s - loss: 30220758949279.9922     \n",
      "Epoch 12/200\n",
      "6602/6602 [==============================] - 0s - loss: 30220354634130.0312     \n",
      "Epoch 13/200\n",
      "6602/6602 [==============================] - 0s - loss: 30220096620903.2188     \n",
      "Epoch 14/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219933555740.8516     \n",
      "Epoch 15/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219831692983.1758     \n",
      "Epoch 16/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219766110129.2070     \n",
      "Epoch 17/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219730283835.4844     \n",
      "Epoch 18/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219699916108.8555     \n",
      "Epoch 19/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219677841694.0117     \n",
      "Epoch 20/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219655462331.2891     \n",
      "Epoch 21/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219633143958.1406     \n",
      "Epoch 22/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219612734050.4922     \n",
      "Epoch 23/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219591577020.5312     \n",
      "Epoch 24/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219570287846.4844     \n",
      "Epoch 25/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219549021543.5312     \n",
      "Epoch 26/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219526835314.4648     \n",
      "Epoch 27/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219506868851.8633     \n",
      "Epoch 28/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219489381361.4180     \n",
      "Epoch 29/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219472674029.3125     \n",
      "Epoch 30/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219456935542.0312     \n",
      "Epoch 31/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219441413059.5078     \n",
      "Epoch 32/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219425305458.2344     \n",
      "Epoch 33/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219410707348.9766     \n",
      "Epoch 34/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219391900958.9453     \n",
      "Epoch 35/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219375019552.4180     \n",
      "Epoch 36/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219359375090.7383     \n",
      "Epoch 37/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219345440878.4336     \n",
      "Epoch 38/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219331269696.2148     \n",
      "Epoch 39/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219316655068.9492     \n",
      "Epoch 40/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219301846672.7109     \n",
      "Epoch 41/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219287380072.2344     \n",
      "Epoch 42/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219274095144.7930     \n",
      "Epoch 43/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219261437901.7461     \n",
      "Epoch 44/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219250468036.9805     \n",
      "Epoch 45/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219239090304.4258     \n",
      "Epoch 46/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219227050580.8398     \n",
      "Epoch 47/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219217341167.3281     \n",
      "Epoch 48/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219208486878.4961     \n",
      "Epoch 49/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219200849839.9648     \n",
      "Epoch 50/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219192622600.2227     \n",
      "Epoch 51/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219187235187.6289     \n",
      "Epoch 52/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219182945587.4180     \n",
      "Epoch 53/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219176032800.1055     \n",
      "Epoch 54/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219171279424.9883     \n",
      "Epoch 55/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219163070609.1758     \n",
      "Epoch 56/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219159084686.2305     \n",
      "Epoch 57/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219158091064.3828     \n",
      "Epoch 58/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219157939861.0547     \n",
      "Epoch 59/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219158239726.4727     \n",
      "Epoch 60/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219158261326.9492     \n",
      "Epoch 61/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219157892848.2578     \n",
      "Epoch 62/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219157680337.6992     \n",
      "Epoch 63/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219157429708.6602     \n",
      "Epoch 64/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219156559336.5820     \n",
      "Epoch 65/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219153565129.5586     \n",
      "Epoch 66/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219154037798.7734     \n",
      "Epoch 67/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219153179497.5508     \n",
      "Epoch 68/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219153203003.9492     \n",
      "Epoch 69/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219153043541.6172     \n",
      "Epoch 70/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219152400609.8320     \n",
      "Epoch 71/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219152399974.5234     \n",
      "Epoch 72/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219151709394.6328     \n",
      "Epoch 73/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219151906975.4492     \n",
      "Epoch 74/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219150422260.4453     \n",
      "Epoch 75/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219148013489.8281     \n",
      "Epoch 76/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219148092585.6875     \n",
      "Epoch 77/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219147567662.3750     \n",
      "Epoch 78/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219147939158.7773     \n",
      "Epoch 79/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219147458548.2109     \n",
      "Epoch 80/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219147551303.1914     \n",
      "Epoch 81/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219147691070.9727     \n",
      "Epoch 82/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219146881053.1602     \n",
      "Epoch 83/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219146985243.6875     \n",
      "Epoch 84/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219145168897.8594     \n",
      "Epoch 85/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219143423071.2344     \n",
      "Epoch 86/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219143755972.6719     \n",
      "Epoch 87/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219143132735.4375     \n",
      "Epoch 88/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219142992967.6562     \n",
      "Epoch 89/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219142146737.2852     \n",
      "Epoch 90/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219141835436.3203     \n",
      "Epoch 91/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219141874190.1133     \n",
      "Epoch 92/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219141418038.9062     \n",
      "Epoch 93/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219141313213.0742     \n",
      "Epoch 94/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219138715438.3008     \n",
      "Epoch 95/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219137435292.5000     \n",
      "Epoch 96/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219137885725.9375     \n",
      "Epoch 97/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219137285995.1016     \n",
      "Epoch 98/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219137308230.8789     \n",
      "Epoch 99/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219137033142.4766     \n",
      "Epoch 100/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219137223734.9062     \n",
      "Epoch 101/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219137026789.3984     \n",
      "Epoch 102/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219136789184.1758     \n",
      "Epoch 103/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219136737088.9102     \n",
      "Epoch 104/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219133580878.3281     \n",
      "Epoch 105/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219133304519.3086     \n",
      "Epoch 106/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219132725118.3320     \n",
      "Epoch 107/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219132321697.6953     \n",
      "Epoch 108/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219132173670.9102     \n",
      "Epoch 109/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219131418924.9023     \n",
      "Epoch 110/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219131684483.6836     \n",
      "Epoch 111/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219130970397.3945     \n",
      "Epoch 112/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219130744227.7109     \n",
      "Epoch 113/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219130512340.2578     \n",
      "Epoch 114/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219127396789.3945     \n",
      "Epoch 115/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219127471755.7500     \n",
      "Epoch 116/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219126879648.6094     \n",
      "Epoch 117/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219126965097.5508     \n",
      "Epoch 118/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219126997180.6094     \n",
      "Epoch 119/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219127243680.1445     \n",
      "Epoch 120/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219126703668.2695     \n",
      "Epoch 121/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219126448909.7266     \n",
      "Epoch 122/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219126529593.8555     \n",
      "Epoch 123/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219125189093.7852     \n",
      "Epoch 124/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219122993469.0352     \n",
      "Epoch 125/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219123086541.6680     \n",
      "Epoch 126/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219122121826.3359     \n",
      "Epoch 127/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219121916939.4805     \n",
      "Epoch 128/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219121885174.0703     \n",
      "Epoch 129/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219121268607.5742     \n",
      "Epoch 130/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219121113910.0508     \n",
      "Epoch 131/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219120796573.6641     \n",
      "Epoch 132/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219120511320.3359     \n",
      "Epoch 133/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219119101571.6836     \n",
      "Epoch 134/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219116955500.9609     \n",
      "Epoch 135/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219117077480.1172     \n",
      "Epoch 136/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219116686765.6406     \n",
      "Epoch 137/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219116371652.8242     \n",
      "Epoch 138/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219116318286.9492     \n",
      "Epoch 139/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219116614975.8242     \n",
      "Epoch 140/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219116314475.1016     \n",
      "Epoch 141/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219115611506.6992     \n",
      "Epoch 142/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219115938055.0547     \n",
      "Epoch 143/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219113658887.2891     \n",
      "Epoch 144/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219112503261.8789     \n",
      "Epoch 145/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219112367941.2539     \n",
      "Epoch 146/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219112108735.5547     \n",
      "Epoch 147/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219111695785.2969     \n",
      "Epoch 148/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219111028076.4961     \n",
      "Epoch 149/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219110481076.2305     \n",
      "Epoch 150/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219110533171.4922     \n",
      "Epoch 151/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219110226953.0000     \n",
      "Epoch 152/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219110120856.5469     \n",
      "Epoch 153/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219107069471.7969     \n",
      "Epoch 154/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219106533907.0781     \n",
      "Epoch 155/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219106382703.7539     \n",
      "Epoch 156/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219106094909.1914     \n",
      "Epoch 157/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219106047896.3906     \n",
      "Epoch 158/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219106020578.1406     \n",
      "Epoch 159/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219106059967.2422     \n",
      "Epoch 160/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219105814738.3203     \n",
      "Epoch 161/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219105600639.4961     \n",
      "Epoch 162/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219105441812.4727     \n",
      "Epoch 163/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219102260189.5664     \n",
      "Epoch 164/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219101718271.7656     \n",
      "Epoch 165/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219101802767.7422     \n",
      "Epoch 166/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219101572150.9062     \n",
      "Epoch 167/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219101091222.6836     \n",
      "Epoch 168/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219100488315.3086     \n",
      "Epoch 169/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219100202109.0156     \n",
      "Epoch 170/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219100231650.8398     \n",
      "Epoch 171/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219099762793.4727     \n",
      "Epoch 172/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219099162427.3281     \n",
      "Epoch 173/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219096414084.5352     \n",
      "Epoch 174/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219096299093.7695     \n",
      "Epoch 175/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219095894402.5195     \n",
      "Epoch 176/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219095931250.3906     \n",
      "Epoch 177/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219095706351.3281     \n",
      "Epoch 178/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219095679350.7305     \n",
      "Epoch 179/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219096033534.9922     \n",
      "Epoch 180/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219095453498.7070     \n",
      "Epoch 181/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219095327707.7070     \n",
      "Epoch 182/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219093863957.8672     \n",
      "Epoch 183/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219091752193.7812     \n",
      "Epoch 184/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219091596543.3008     \n",
      "Epoch 185/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219091094014.6055     \n",
      "Epoch 186/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219091248394.4727     \n",
      "Epoch 187/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219090634051.5469     \n",
      "Epoch 188/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219089696972.1172     \n",
      "Epoch 189/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219089958083.7422     \n",
      "Epoch 190/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219089693795.5742     \n",
      "Epoch 191/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219089498120.6875     \n",
      "Epoch 192/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219087396521.5312     \n",
      "Epoch 193/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219086008373.3555     \n",
      "Epoch 194/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219086027432.6016     \n",
      "Epoch 195/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219085643706.5156     \n",
      "Epoch 196/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219085539515.9883     \n",
      "Epoch 197/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219085287298.6758     \n",
      "Epoch 198/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219085385771.4297     \n",
      "Epoch 199/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219085301275.4531     \n",
      "Epoch 200/200\n",
      "6602/6602 [==============================] - 0s - loss: 30219085024122.3008     \n",
      "Epoch 1/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219084449327.3086     \n",
      "Epoch 2/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219082028803.4883     \n",
      "Epoch 3/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219081263257.2461     \n",
      "Epoch 4/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219080957038.7461     \n",
      "Epoch 5/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219080791223.3320     \n",
      "Epoch 6/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219080756281.3906     \n",
      "Epoch 7/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219079767106.6953     \n",
      "Epoch 8/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219079482488.6719     \n",
      "Epoch 9/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219079550784.2891     \n",
      "Epoch 10/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219078700424.4180     \n",
      "Epoch 11/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219079168646.4727     \n",
      "Epoch 12/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219075836455.5508     \n",
      "Epoch 13/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219075516895.5820     \n",
      "Epoch 14/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219075305337.9922     \n",
      "Epoch 15/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219075437482.0742     \n",
      "Epoch 16/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219074815515.4531     \n",
      "Epoch 17/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219075151593.4336     \n",
      "Epoch 18/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219075055026.6016     \n",
      "Epoch 19/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219074503579.1836     \n",
      "Epoch 20/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219074417177.2852     \n",
      "Epoch 21/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219074123664.9453     \n",
      "Epoch 22/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219070928065.2617     \n",
      "Epoch 23/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219071196165.2734     \n",
      "Epoch 24/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219070088187.9688     \n",
      "Epoch 25/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219070288310.0117     \n",
      "Epoch 26/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219070132024.2266     \n",
      "Epoch 27/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219067937670.0859     \n",
      "Epoch 28/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219068000565.5859     \n",
      "Epoch 29/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219066660700.8320     \n",
      "Epoch 30/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219066615276.3047     \n",
      "Epoch 31/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219065715997.7031     \n",
      "Epoch 32/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219062677954.4219     \n",
      "Epoch 33/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219063083280.9844     \n",
      "Epoch 34/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219062477197.0703     \n",
      "Epoch 35/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219062631576.9336     \n",
      "Epoch 36/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219062420336.9922     \n",
      "Epoch 37/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219062394607.0156     \n",
      "Epoch 38/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219062545175.0312     \n",
      "Epoch 39/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219061977209.6055     \n",
      "Epoch 40/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219062086323.7656     \n",
      "Epoch 41/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219060122745.2930     \n",
      "Epoch 42/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219058515098.1758     \n",
      "Epoch 43/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219058581487.8711     \n",
      "Epoch 44/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219057627255.1172     \n",
      "Epoch 45/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219057953803.4805     \n",
      "Epoch 46/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219057153315.2852     \n",
      "Epoch 47/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219057120914.5742     \n",
      "Epoch 48/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219056631727.3477     \n",
      "Epoch 49/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219056516736.5820     \n",
      "Epoch 50/50\n",
      "6602/6602 [==============================] - 0s - loss: 30219056284213.8203     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f398df30cd0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN1.fit(X1,Y1, batch_size=batch_size1, epochs=200, verbose=0)\n",
    "NN1.fit(X1,Y1, batch_size=batch_size1, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NN1.save('nn1.h5')\n",
    "#NN1=load_model('nn1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.32262013006e+25 -3.5981464011\n"
     ]
    }
   ],
   "source": [
    "print r2_score(NN1.predict(X1),Y1),r2_score(NN2.predict(X2),Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[data_test1[\"price_doc\"], data_test2[\"price_doc\"]] = [NN1.predict(X1_test), NN2.predict(X2_test)]\n",
    "out = data_test1[[\"id\",\"price_doc\"]].append(data_test2[[\"id\",\"price_doc\"]])\n",
    "out.to_csv(\"./nn2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_predict(data, data_test, ex, d, rs1=1, rs2=1, ts=0.1, md=5, sl=10, ne=100):\n",
    "    v=[x for x in list(data_test.columns.values) if x not in ex]\n",
    "    [values, features, features_test]  = [data[d], data[v], data_test[v]]\n",
    "    train, test, y_train, y_test = train_test_split(features, values, test_size=ts, random_state=rs1)\n",
    "    RFreg=RandomForestRegressor(random_state=rs2, max_depth=md, min_samples_leaf=sl, n_estimators=ne, oob_score=True)\n",
    "    RFreg.fit(train,y_train)\n",
    "    pred1 = RFreg.predict(test)\n",
    "    print r2_score(y_test, pred1)\n",
    "    pred_f=RFreg.predict(features_test)\n",
    "    return features_test[\"full_sq\"]*pred_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex=[\"id\",\"timestamp\",\"sub_area\", \"ID_metro\", \"ID_railroad_station_walk\", \"ID_railroad_station_avto\", \n",
    "    \"ID_big_road1\", \"ID_big_road2\", \"ID_railroad_terminal\", \"ID_bus_terminal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "def my_hist(data, d):\n",
    "    x=data[d].as_matrix()\n",
    "    bins = np.arange(0,3e5,1e4)\n",
    "    plt.hist(x, facecolor='green', alpha=0.75, bins=bins)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEECAYAAADpigmnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK5SzSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f702a08afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = \"pr_RUB_SQM\"\n",
    "my_hist(data1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEECAYAAADpigmnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f702a08a190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_hist(data2,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.505425669374\n",
      "-0.199517942226\n"
     ]
    }
   ],
   "source": [
    "[data_test1[\"price_doc\"], data_test2[\"price_doc\"]] = [fit_predict(data1, data_test1, ex, d, rs1=3, sl=5),\n",
    "                                                      fit_predict(data2, data_test2, ex, d, rs1=5, sl=10)]\n",
    "out = data_test1[[\"id\",\"price_doc\"]].append(data_test2[[\"id\",\"price_doc\"]])\n",
    "out.to_csv(\"./result2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ColumnApplier(object):\n",
    "    def __init__(self, column_stages):\n",
    "        self._column_stages = column_stages\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i, k in self._column_stages.items():\n",
    "            k.fit(X[:, i])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for i, k in self._column_stages.items():\n",
    "            X[:, i] = k.transform(X[:, i])\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
