{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filenames):\n",
    "    arrays = [0,0,0]\n",
    "    for i in xrange(len(filenames)):\n",
    "        arrays[i] = pd.read_csv(filenames[i], parse_dates=['timestamp'])\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_transport(data):\n",
    "    data[\"metro_km_walk\"]=data[\"metro_km_walk\"].fillna(data[\"metro_km_avto\"])\n",
    "    data[\"metro_min_walk\"]=data[\"metro_min_walk\"].fillna(data[\"metro_km_avto\"]*12)\n",
    "    data[\"railroad_station_walk_km\"]=data[\"railroad_station_walk_km\"].fillna(data[\"railroad_station_avto_km\"])\n",
    "    data[\"railroad_station_walk_min\"]=data[\"railroad_station_walk_min\"].fillna(data[\"railroad_station_avto_km\"]*12)\n",
    "    data[\"ID_railroad_station_walk\"]=data[\"ID_railroad_station_walk\"].fillna(data[\"ID_railroad_station_avto\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "def my_cafe(data):\n",
    "    st0=['cafe_sum_5000_min_price_avg','cafe_sum_5000_max_price_avg','cafe_avg_price_5000']\n",
    "    l=['5000','3000','2000','1500','1000','500']\n",
    "    medImputer=Imputer(strategy='median')\n",
    "    data[st0]=medImputer.fit_transform(data[st0])\n",
    "    for i in xrange(len(l)-1):\n",
    "        st1=[st.replace('5000',str(l[i+1])) for st in st0]\n",
    "        st2=[st.replace('5000',str(l[i])) for st in st0]\n",
    "        for k in xrange(len(st1)):\n",
    "            data[st1[k]]=data[st1[k]].fillna(data[st2[k]])\n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def er10(data):\n",
    "    err = (data.full_sq>data.life_sq*10)&(data.life_sq>2)\n",
    "    data.ix[err,'full_sq'] = data.ix[err,'full_sq']/10\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_impute(data):\n",
    "    data[\"floor\"]=data[\"floor\"].fillna(data[\"max_floor\"])\n",
    "    data[\"life_sq\"]=(data[\"life_sq\"]/data[\"full_sq\"]).replace(np.inf,1)\n",
    "    data[\"kitch_sq\"]=(data[\"kitch_sq\"]/data[\"full_sq\"]).replace(np.inf,1)\n",
    "    data[\"max_floor\"]=(data[\"max_floor\"]/data[\"floor\"]).replace(np.inf,1)\n",
    "    data[\"num_room\"]=(data[\"num_room\"]/data[\"full_sq\"]).replace(np.inf,1)\n",
    "    data.replace(np.inf, np.nan)\n",
    "    data['green_part_2000']=data['green_part_2000'].fillna(data['green_part_1500'])\n",
    "    data['prom_part_5000']=data['prom_part_5000'].fillna(data['prom_part_2000'])\n",
    "    #data=data.apply(yes_1)\n",
    "    medImputer=Imputer(strategy='median')\n",
    "    mfImputer=Imputer(strategy='most_frequent')\n",
    "    data[[\"material\",\"build_year\",\"state\",\"floor\"]]=mfImputer.fit_transform(data[[\"material\",\"build_year\",\"state\",\"floor\"]])\n",
    "    data[[\"life_sq\",\"max_floor\",\"num_room\",\"kitch_sq\"]]=medImputer.fit_transform(data[[\"life_sq\",\"max_floor\",\"num_room\",\"kitch_sq\"]])\n",
    "    data[\"build_year\"]=2017-data[\"build_year\"]\n",
    "    return data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_features(train):\n",
    "    \n",
    "    #train['rel_floor'] = train['floor'] / train['max_floor'].astype(float)\n",
    "    #train['rel_kitch_sq'] = train['kitch_sq'] / train['full_sq'].astype(float)\n",
    "    train.apartment_name=train.sub_area + train['metro_km_avto'].astype(str)\n",
    "    #train['room_size'] = 1 / train['num_room'].astype(float)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labels(x_train):   \n",
    "    for c in x_train.columns:\n",
    "        if x_train[c].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(x_train[c].values))\n",
    "            x_train[c] = lbl.transform(list(x_train[c].values))\n",
    "            #print c\n",
    "            #x_train.drop(c,axis=1,inplace=True)    \n",
    "    return x_train   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_dates(train):\n",
    "    \n",
    "    # Add month-year\n",
    "    month_year = (train.timestamp.dt.month + train.timestamp.dt.year * 100)\n",
    "    month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "    train['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "    \n",
    "    # Add week-year count\n",
    "    week_year = (train.timestamp.dt.weekofyear + train.timestamp.dt.year * 100)\n",
    "    week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "    train['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "    \n",
    "    # Add month and day-of-week\n",
    "    train['month'] = train.timestamp.dt.month\n",
    "    train['dow'] = train.timestamp.dt.dayofweek\n",
    "    \n",
    "    return train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printna(data):\n",
    "    k=len(data)-data.count()\n",
    "    return k[k>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sq(train):\n",
    "    bad_index = train[(train.life_sq > train.full_sq) | \n",
    "                      (train.life_sq < 5)].index\n",
    "    train.ix[bad_index, \"life_sq\"] = np.NaN\n",
    "\n",
    "    bad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3) | \n",
    "                      (train.full_sq < 5)].index\n",
    "    train.ix[bad_index, \"full_sq\"] = np.NaN\n",
    "    \n",
    "    bad_index = train[(train.kitch_sq >= train.life_sq) |\n",
    "                      (train.kitch_sq == 0) | \n",
    "                      (train.kitch_sq == 1)].index\n",
    "    train.ix[bad_index, \"kitch_sq\"] = np.NaN\n",
    "    \n",
    "    bad_index = train[train.life_sq > 300].index\n",
    "    train.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n",
    "    \n",
    "    bad_index = train[train.build_year < 1500].index\n",
    "    train.ix[bad_index, \"build_year\"] = np.NaN\n",
    "    \n",
    "    bad_index = train[train.num_room == 0].index \n",
    "    train.ix[bad_index, \"num_room\"] = np.NaN\n",
    "    \n",
    "    bad_index = train[train.max_floor == 0].index\n",
    "    train.ix[bad_index, \"max_floor\"] = np.NaN\n",
    "    \n",
    "    bad_index = train[train.floor > train.max_floor].index\n",
    "    train.ix[bad_index, \"max_floor\"] = np.NaN    \n",
    "    \n",
    "    return(train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bad_address(data):\n",
    "    data=data.set_index('id')\n",
    "    fx = pd.read_excel('./data/BAD_ADDRESS_FIX.xlsx').drop_duplicates('id').set_index('id')\n",
    "    data.update(fx)\n",
    "    print('Fix: ', data.index.intersection(fx.index).shape[0])\n",
    "    return data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def month_median(macro):\n",
    "    macro[\"year\"]  = macro[\"timestamp\"].dt.year\n",
    "    macro[\"month\"] = macro[\"timestamp\"].dt.month\n",
    "    macro[\"yearmonth\"] = 100*macro.year + macro.month\n",
    "    macmeds = macro.groupby(\"yearmonth\").median()\n",
    "    return macmeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "micro_humility_factor = 1     #    range from 0 (complete humility) to 1 (no humility)\n",
    "macro_humility_factor = 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = [\"./data/macro.csv\",\"./data/train.csv/train.csv\",\"./data/test.csv/test.csv\"]\n",
    "[macro, train, test] = read_data(filenames)\n",
    "[macro, train] = map(month_median,[macro, train])\n",
    "id_test = test.id\n",
    "df = macro.join(train, lsuffix='_left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.matlib as ml\n",
    "import seaborn as sns \n",
    "def almonZmatrix(X, maxlag, maxdeg):\n",
    "    \"\"\"\n",
    "    Creates the Z matrix corresponding to vector X.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    Z = ml.zeros((len(X)-maxlag, maxdeg+1))\n",
    "    for t in range(maxlag,  n):\n",
    "       #Solve for Z[t][0].\n",
    "       Z[t-maxlag,0] = sum([X[t-lag] for lag in range(maxlag+1)])\n",
    "       for j in range(1, maxdeg+1):\n",
    "             s = 0.0\n",
    "             for i in range(1, maxlag+1):       \n",
    "                s += (i)**j * X[t-i]\n",
    "             Z[t-maxlag,j] = s\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6593788.9177231779"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for macro model\n",
    "import statsmodels.api as sm\n",
    "y = df.price_doc.div(df.cpi).apply(np.log).loc[201108:201506]\n",
    "lncpi = df.cpi.apply(np.log)\n",
    "tblags = 5    # Number of lags used on PDL for Trade Balance\n",
    "mrlags = 5    # Number of lags used on PDL for Mortgage Rate\n",
    "cplags = 5    # Number of lags used on PDL for CPI\n",
    "ztb = almonZmatrix(df.balance_trade.loc[201103:201506].as_matrix(), tblags, 1)\n",
    "zmr = almonZmatrix(df.mortgage_rate.loc[201103:201506].as_matrix(), mrlags, 1)\n",
    "zcp = almonZmatrix(lncpi.loc[201103:201506].as_matrix(), cplags, 1)\n",
    "columns = ['tb0', 'tb1', 'mr0', 'mr1', 'cp0', 'cp1']\n",
    "z = pd.DataFrame( np.concatenate( (ztb, zmr, zcp), axis=1), y.index.values, columns )\n",
    "X = sm.add_constant( z )\n",
    "\n",
    "# Fit macro model\n",
    "eq = sm.OLS(y, X)\n",
    "fit = eq.fit()\n",
    "\n",
    "# Predict with macro model\n",
    "test_cpi = df.cpi.loc[201507:201605]\n",
    "test_index = test_cpi.index\n",
    "ztb_test = almonZmatrix(df.balance_trade.loc[201502:201605].as_matrix(), tblags, 1)\n",
    "zmr_test = almonZmatrix(df.mortgage_rate.loc[201502:201605].as_matrix(), mrlags, 1)\n",
    "zcp_test = almonZmatrix(lncpi.loc[201502:201605].as_matrix(), cplags, 1)\n",
    "z_test = pd.DataFrame( np.concatenate( (ztb_test, zmr_test, zcp_test), axis=1), \n",
    "                       test_index, columns )\n",
    "X_test = sm.add_constant( z_test )\n",
    "pred_lnrp = fit.predict( X_test )\n",
    "pred_p = np.exp(pred_lnrp) * test_cpi\n",
    "\n",
    "# Merge with test cases and compute mean for macro prediction\n",
    "test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n",
    "test[\"year\"]  = test[\"timestamp\"].dt.year\n",
    "test[\"month\"] = test[\"timestamp\"].dt.month\n",
    "test[\"yearmonth\"] = 100*test.year + test.month\n",
    "test_ids = test[[\"yearmonth\",\"id\"]]\n",
    "monthprices = pd.DataFrame({\"yearmonth\":pred_p.index.values,\"monthprice\":pred_p.values})\n",
    "macro_mean = np.exp(test_ids.merge(monthprices, on=\"yearmonth\").monthprice.apply(np.log).mean())\n",
    "macro_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7773440.7512487005"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive macro model assumes housing prices will simply follow CPI\n",
    "naive_pred_lnrp = y.mean()\n",
    "naive_pred_p = np.exp(naive_pred_lnrp) * test_cpi\n",
    "monthnaive = pd.DataFrame({\"yearmonth\":pred_p.index.values, \"monthprice\":naive_pred_p.values})\n",
    "macro_naive = np.exp(test_ids.merge(monthnaive, on=\"yearmonth\").monthprice.apply(np.log).mean())\n",
    "macro_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6637341.6089008758"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine naive and substantive macro models\n",
    "macro_mean = macro_naive * (macro_mean/macro_naive) ** macro_humility_factor\n",
    "macro_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fix: ', 550)\n",
      "('Fix: ', 149)\n"
     ]
    }
   ],
   "source": [
    "filenames = [\"./data/macro.csv\",\"./data/train.csv/train.csv\",\"./data/test.csv/test.csv\"]\n",
    "[macro, train, test] = read_data(filenames)\n",
    "train, test = map(bad_address, [train, test])\n",
    "train=er10(er10(train))\n",
    "train, test = map(my_transport, [train, test])\n",
    "train, test = map(my_cafe, [train, test])\n",
    "id_test = test.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    2662\n",
       "1.0    2266\n",
       "3.0    1913\n",
       "4.0     127\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean data\n",
    "equal_index = [601,1896,2791]\n",
    "test.ix[equal_index, \"life_sq\"] = test.ix[equal_index, \"full_sq\"]\n",
    "\n",
    "kitch_is_build_year = [13117]\n",
    "train.ix[kitch_is_build_year, \"build_year\"] = train.ix[kitch_is_build_year, \"kitch_sq\"]\n",
    "\n",
    "bad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\n",
    "train.ix[bad_index, \"num_room\"] = np.NaN\n",
    "\n",
    "bad_index = [3174, 7313]\n",
    "test.ix[bad_index, \"num_room\"] = np.NaN\n",
    "\n",
    "[train, test] = [clean_sq(train), clean_sq(test)]\n",
    "\n",
    "train.product_type.value_counts(normalize= True)\n",
    "test.product_type.value_counts(normalize= True)\n",
    "\n",
    "\n",
    "train.floor.describe(percentiles= [0.9999])\n",
    "bad_index = [23584]\n",
    "train.ix[bad_index, \"floor\"] = np.NaN\n",
    "train.material.value_counts()\n",
    "test.material.value_counts()\n",
    "train.state.value_counts()\n",
    "bad_index = train[train.state == 33].index\n",
    "train.ix[bad_index, \"state\"] = np.NaN\n",
    "test.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# brings error down a lot by removing extreme price per sqm\n",
    "train.loc[train.full_sq == 0, 'full_sq'] = 50\n",
    "train = train[train.price_doc/train.full_sq <= 600000]\n",
    "train = train[train.price_doc/train.full_sq >= 10000]\n",
    "#mult = .969 train[\"price_doc\"] * mult + 10\n",
    "train[\"persqm\"] = train[\"price_doc\"]/train.full_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[train, test] = [add_dates(train), add_dates(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Other feature engineering\n",
    "train, test = my_impute(train), my_impute(test)\n",
    "train, test = new_features(train), new_features(test)\n",
    "v=['hospital_beds_raion']\n",
    "train, test = train.drop(v, axis=1), test.drop(v, axis=1)\n",
    "test.full_sq = test.full_sq.fillna(1.5*test.life_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = train[\"persqm\"]\n",
    "x_train = train.drop([\"id\", \"timestamp\", \"price_doc\", \"persqm\"], axis=1)\n",
    "x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n",
    "\n",
    "x_train, x_test = labels(x_train), labels(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v=printna(x_train).index\n",
    "#x_train, x_test = x_train.drop(v, axis=1), x_test.drop(v, axis=1)\n",
    "x_train, x_test = x_train.fillna(0), x_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=np.array(x_train)\n",
    "Y=np.array(y_train)\n",
    "X_test=np.array(x_test)\n",
    "Norm = StandardScaler()\n",
    "Xt =Norm.fit_transform(X)\n",
    "Xt_test=Norm.transform(X_test)\n",
    "Yt = (Y - np.mean(Y))/(max(Y)-min(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import seaborn as sns\n",
    "from sklearn import model_selection, preprocessing\n",
    "import xgboost as xgb\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(Xt, Yt)\n",
    "dtest = xgb.DMatrix(Xt_test)\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id     price_doc\n",
      "0  30474  5.621197e+06\n",
      "1  30475  8.322257e+06\n",
      "2  30476  5.379540e+06\n",
      "3  30477  5.647403e+06\n",
      "4  30478  4.852786e+06\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(dtest)\n",
    "Y_test = ((max(Y)-min(Y))*y_predict + np.mean(Y))*test[\"full_sq\"]\n",
    "result = pd.DataFrame({'id': id_test, 'price_doc': Y_test})\n",
    "print result.head()\n",
    "result.to_csv('unadj.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adjust\n",
    "\n",
    "lny = np.log(result.price_doc)\n",
    "lnm = np.log(macro_mean)\n",
    "\n",
    "# I'm not sure whether this makes any sense or not.\n",
    "# 1+lny.mean()-lnm term is meant to offest the scale effect of the logarithmic mean shift\n",
    "#   while allowing the new logarithmic mean to remain at lnm.\n",
    "y_trans = lnm  +  micro_humility_factor * (lny-lny.mean()) * (1+lny.mean()-lnm)\n",
    "y_predict = np.exp( y_trans )\n",
    "\n",
    "y_predict = np.round(y_predict * 0.99)\n",
    "\n",
    "sub = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n",
    "sub.head()\n",
    "sub.to_csv('adj.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def larger_model1(dim, weight_reg=0.01, activ_reg=0.01, dp=0.2):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim, input_dim=dim, kernel_initializer='random_normal', activation='linear', \n",
    "                    kernel_regularizer=regularizers.l2(weight_reg),activity_regularizer=regularizers.l1(activ_reg)))\n",
    "    model.add(Dropout(dp))\n",
    "    #model.add(Dense(200, kernel_initializer='random_normal', activation='relu',\n",
    "                   #kernel_regularizer=regularizers.l2(weight_reg),activity_regularizer=regularizers.l1(activ_reg)))\n",
    "    #model.add(Dropout(dp))\n",
    "    model.add(Dense(1, kernel_initializer='random_normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def larger_model(dim, reg=0.01):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim, input_dim=dim, kernel_initializer='random_normal', activation='relu', \n",
    "                    kernel_regularizer=regularizers.l2(reg),activity_regularizer=regularizers.l1(reg)))\n",
    "    model.add(Dense(148, kernel_initializer='random_normal', activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(reg),activity_regularizer=regularizers.l1(reg)))\n",
    "    model.add(Dense(20, kernel_initializer='random_normal', activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(reg),activity_regularizer=regularizers.l1(reg)))\n",
    "    model.add(Dense(1, kernel_initializer='random_normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN = larger_model(dim=Xt.shape[1]) #, weight_reg=10, activ_reg=10, dp=0.2)\n",
    "#NN = load_model('models/drop274r10-0609.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30403/30403 [==============================] - 0s - loss: 20683894727.6856     \n",
      "Epoch 2/5\n",
      "30403/30403 [==============================] - 0s - loss: 20683878338.9702     \n",
      "Epoch 3/5\n",
      "30403/30403 [==============================] - 0s - loss: 20683862090.1653     \n",
      "Epoch 4/5\n",
      "30403/30403 [==============================] - 0s - loss: 20683845919.2306     \n",
      "Epoch 5/5\n",
      "30403/30403 [==============================] - 0s - loss: 20683829682.2140     \n",
      "60 0.0\n",
      "Epoch 1/5\n",
      "30403/30403 [==============================] - 0s - loss: 20682191102.4928     \n",
      "Epoch 2/5\n",
      "30403/30403 [==============================] - 0s - loss: 20682174921.9253     \n",
      "Epoch 3/5\n",
      "30403/30403 [==============================] - 0s - loss: 20682158985.5443     \n",
      "Epoch 4/5\n",
      "30403/30403 [==============================] - 0s - loss: 20682142442.8400     \n",
      "Epoch 5/5\n",
      "30403/30403 [==============================] - 0s - loss: 20682126296.4923     \n",
      "60 -2.52669231123e+21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-2678df23c0d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2229\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/serzzh/anaconda2/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq=[60,60,60]\n",
    "for i in seq:\n",
    "    NN.fit(X,Y, batch_size=X.shape[0]/i, epochs=100, verbose=0)\n",
    "    NN.fit(X,Y, batch_size=X.shape[0]/i, epochs=5, verbose=1)\n",
    "    print i, r2_score(NN.predict(X),Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.253640320727\n"
     ]
    }
   ],
   "source": [
    "print r2_score(NN.predict(X),Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN.save('models/drop274r10-0609.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Yt_test = NN.predict(Xt_test)\n",
    "test['persqm'] = (max(Y)-min(Y))*Yt_test + np.mean(Y)\n",
    "test[\"price_doc\"] =test['persqm']*test[\"full_sq\"]\n",
    "#out = test[[\"id\",\"price_doc\"]]\n",
    "#out.to_csv(\"./nn2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5.404131e+07\n",
       "1       8.171049e+06\n",
       "2      -4.924844e+05\n",
       "3       6.487564e+07\n",
       "4       2.382602e+06\n",
       "5       2.264605e+06\n",
       "6       1.608603e+06\n",
       "7       1.845307e+06\n",
       "8       9.914066e+06\n",
       "9       1.040796e+08\n",
       "10     -7.071814e+05\n",
       "11      2.178462e+06\n",
       "12      2.141564e+06\n",
       "13      8.625219e+05\n",
       "14      3.829185e+08\n",
       "15      3.430434e+06\n",
       "16      1.514366e+06\n",
       "17      2.136032e+06\n",
       "18      9.459892e+05\n",
       "19      9.525724e+06\n",
       "20      2.119675e+06\n",
       "21      7.270033e+05\n",
       "22      8.490811e+05\n",
       "23      1.606674e+06\n",
       "24     -2.760079e+06\n",
       "25     -6.631009e+05\n",
       "26      2.644038e+05\n",
       "27     -1.647346e+05\n",
       "28      1.337572e+06\n",
       "29      1.649194e+06\n",
       "            ...     \n",
       "7632   -4.221885e+05\n",
       "7633    7.391184e+05\n",
       "7634    2.349940e+07\n",
       "7635    1.377595e+06\n",
       "7636    2.859038e+08\n",
       "7637    1.478190e+08\n",
       "7638    2.641069e+08\n",
       "7639   -5.094874e+05\n",
       "7640    2.265302e+07\n",
       "7641    2.705324e+08\n",
       "7642    1.053888e+06\n",
       "7643    3.511938e+06\n",
       "7644    3.348321e+05\n",
       "7645    2.325257e+08\n",
       "7646   -4.015542e+04\n",
       "7647    3.852036e+06\n",
       "7648    8.566405e+08\n",
       "7649    6.117122e+07\n",
       "7650    2.137614e+06\n",
       "7651    3.084420e+06\n",
       "7652    3.692482e+06\n",
       "7653    8.836969e+08\n",
       "7654    1.815318e+08\n",
       "7655    9.985809e+05\n",
       "7656   -4.282648e+04\n",
       "7657    5.034691e+07\n",
       "7658    1.296119e+08\n",
       "7659    3.254510e+07\n",
       "7660    1.112030e+05\n",
       "7661   -8.062844e+03\n",
       "Name: persqm, dtype: float32"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['persqm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# evaluate model with standardized dataset\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=50, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=2, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='r2')\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
