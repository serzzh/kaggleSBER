{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filenames):\n",
    "    arrays = [0,0,0]\n",
    "    for i in xrange(len(filenames)):\n",
    "        arrays[i] = pd.read_csv(filenames[i], parse_dates=['timestamp'])\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_transport(data):\n",
    "    data[\"metro_km_walk\"]=data[\"metro_km_walk\"].fillna(data[\"metro_km_avto\"])\n",
    "    data[\"metro_min_walk\"]=data[\"metro_min_walk\"].fillna(data[\"metro_km_avto\"]*12)\n",
    "    data[\"railroad_station_walk_km\"]=data[\"railroad_station_walk_km\"].fillna(data[\"railroad_station_avto_km\"])\n",
    "    data[\"railroad_station_walk_min\"]=data[\"railroad_station_walk_min\"].fillna(data[\"railroad_station_avto_km\"]*12)\n",
    "    data[\"ID_railroad_station_walk\"]=data[\"ID_railroad_station_walk\"].fillna(data[\"ID_railroad_station_avto\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "def my_cafe(data):\n",
    "    st0=['cafe_sum_5000_min_price_avg','cafe_sum_5000_max_price_avg','cafe_avg_price_5000']\n",
    "    l=['5000','3000','2000','1500','1000','500']\n",
    "    medImputer=Imputer(strategy='median')\n",
    "    data[st0]=medImputer.fit_transform(data[st0])\n",
    "    for i in xrange(len(l)-1):\n",
    "        st1=[st.replace('5000',str(l[i+1])) for st in st0]\n",
    "        st2=[st.replace('5000',str(l[i])) for st in st0]\n",
    "        for k in xrange(len(st1)):\n",
    "            data[st1[k]]=data[st1[k]].fillna(data[st2[k]])\n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def er10(data):\n",
    "    err = (data.full_sq>data.life_sq*10)&(data.life_sq>2)\n",
    "    data.ix[err,'full_sq'] = data.ix[err,'full_sq']/10\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_impute(data):\n",
    "    data[\"floor\"]=data[\"floor\"].fillna(data[\"max_floor\"])\n",
    "    data[\"life_sq\"]=(data[\"life_sq\"]/data[\"full_sq\"]).replace(np.inf,1)\n",
    "    data[\"kitch_sq\"]=(data[\"kitch_sq\"]/data[\"full_sq\"]).replace(np.inf,1)\n",
    "    data[\"max_floor\"]=(data[\"max_floor\"]/data[\"floor\"]).replace(np.inf,1)\n",
    "    data[\"num_room\"]=(data[\"num_room\"]/data[\"full_sq\"]).replace(np.inf,1)\n",
    "    data.replace(np.inf, np.nan)\n",
    "    data['green_part_2000']=data['green_part_2000'].fillna(data['green_part_1500'])\n",
    "    data['prom_part_5000']=data['prom_part_5000'].fillna(data['prom_part_2000'])\n",
    "    #data=data.apply(yes_1)\n",
    "    medImputer=Imputer(strategy='median')\n",
    "    mfImputer=Imputer(strategy='most_frequent')\n",
    "    data[[\"material\",\"build_year\",\"state\",\"floor\"]]=mfImputer.fit_transform(data[[\"material\",\"build_year\",\"state\",\"floor\"]])\n",
    "    data[[\"life_sq\",\"max_floor\",\"num_room\",\"kitch_sq\"]]=medImputer.fit_transform(data[[\"life_sq\",\"max_floor\",\"num_room\",\"kitch_sq\"]])\n",
    "    data[\"build_year\"]=2017-data[\"build_year\"]\n",
    "    return data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_features(train):\n",
    "    \n",
    "    #train['rel_floor'] = train['floor'] / train['max_floor'].astype(float)\n",
    "    #train['rel_kitch_sq'] = train['kitch_sq'] / train['full_sq'].astype(float)\n",
    "    train.apartment_name=train.sub_area + train['metro_km_avto'].astype(str)\n",
    "    #train['room_size'] = 1 / train['num_room'].astype(float)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labels(x_train):   \n",
    "    for c in x_train.columns:\n",
    "        if x_train[c].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(x_train[c].values))\n",
    "            x_train[c] = lbl.transform(list(x_train[c].values))\n",
    "            #print c\n",
    "            #x_train.drop(c,axis=1,inplace=True)    \n",
    "    return x_train   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_dates(train):\n",
    "    \n",
    "    # Add month-year\n",
    "    month_year = (train.timestamp.dt.month + train.timestamp.dt.year * 100)\n",
    "    month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "    train['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "    \n",
    "    # Add week-year count\n",
    "    week_year = (train.timestamp.dt.weekofyear + train.timestamp.dt.year * 100)\n",
    "    week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "    train['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "    \n",
    "    # Add month and day-of-week\n",
    "    train['month'] = train.timestamp.dt.month\n",
    "    train['dow'] = train.timestamp.dt.dayofweek\n",
    "    \n",
    "    return train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printna(data):\n",
    "    k=len(data)-data.count()\n",
    "    return k[k>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sq(train):\n",
    "    bad_index = train[(train.life_sq > train.full_sq) | \n",
    "                      (train.life_sq < 5)].index\n",
    "    train.ix[bad_index, \"life_sq\"] = np.NaN\n",
    "\n",
    "    bad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3) | \n",
    "                      (train.full_sq < 5)].index\n",
    "    train.ix[bad_index, \"full_sq\"] = np.NaN\n",
    "    \n",
    "    bad_index = train[(train.kitch_sq >= train.life_sq) |\n",
    "                      (train.kitch_sq == 0) | \n",
    "                      (train.kitch_sq == 1)].index\n",
    "    train.ix[bad_index, \"kitch_sq\"] = np.NaN\n",
    "    \n",
    "    bad_index = train[train.life_sq > 300].index\n",
    "    train.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n",
    "    \n",
    "    bad_index = train[train.build_year < 1500].index\n",
    "    train.ix[bad_index, \"build_year\"] = np.NaN\n",
    "    \n",
    "    bad_index = train[train.num_room == 0].index \n",
    "    train.ix[bad_index, \"num_room\"] = np.NaN\n",
    "    \n",
    "    bad_index = train[train.max_floor == 0].index\n",
    "    train.ix[bad_index, \"max_floor\"] = np.NaN\n",
    "    \n",
    "    bad_index = train[train.floor > train.max_floor].index\n",
    "    train.ix[bad_index, \"max_floor\"] = np.NaN    \n",
    "    \n",
    "    return(train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bad_address(data):\n",
    "    data=data.set_index('id')\n",
    "    fx = pd.read_excel('./data/BAD_ADDRESS_FIX.xlsx').drop_duplicates('id').set_index('id')\n",
    "    data.update(fx)\n",
    "    print('Fix: ', data.index.intersection(fx.index).shape[0])\n",
    "    return data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def month_median(macro):\n",
    "    macro[\"year\"]  = macro[\"timestamp\"].dt.year\n",
    "    macro[\"month\"] = macro[\"timestamp\"].dt.month\n",
    "    macro[\"yearmonth\"] = 100*macro.year + macro.month\n",
    "    macmeds = macro.groupby(\"yearmonth\").median()\n",
    "    return macmeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "micro_humility_factor = 1     #    range from 0 (complete humility) to 1 (no humility)\n",
    "macro_humility_factor = 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = [\"./data/macro.csv\",\"./data/train.csv/train.csv\",\"./data/test.csv/test.csv\"]\n",
    "[macro, train, test] = read_data(filenames)\n",
    "[macro, train] = map(month_median,[macro, train])\n",
    "id_test = test.id\n",
    "df = macro.join(train, lsuffix='_left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.matlib as ml\n",
    "import seaborn as sns \n",
    "def almonZmatrix(X, maxlag, maxdeg):\n",
    "    \"\"\"\n",
    "    Creates the Z matrix corresponding to vector X.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    Z = ml.zeros((len(X)-maxlag, maxdeg+1))\n",
    "    for t in range(maxlag,  n):\n",
    "       #Solve for Z[t][0].\n",
    "       Z[t-maxlag,0] = sum([X[t-lag] for lag in range(maxlag+1)])\n",
    "       for j in range(1, maxdeg+1):\n",
    "             s = 0.0\n",
    "             for i in range(1, maxlag+1):       \n",
    "                s += (i)**j * X[t-i]\n",
    "             Z[t-maxlag,j] = s\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6593788.9177231779"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for macro model\n",
    "import statsmodels.api as sm\n",
    "y = df.price_doc.div(df.cpi).apply(np.log).loc[201108:201506]\n",
    "lncpi = df.cpi.apply(np.log)\n",
    "tblags = 5    # Number of lags used on PDL for Trade Balance\n",
    "mrlags = 5    # Number of lags used on PDL for Mortgage Rate\n",
    "cplags = 5    # Number of lags used on PDL for CPI\n",
    "ztb = almonZmatrix(df.balance_trade.loc[201103:201506].as_matrix(), tblags, 1)\n",
    "zmr = almonZmatrix(df.mortgage_rate.loc[201103:201506].as_matrix(), mrlags, 1)\n",
    "zcp = almonZmatrix(lncpi.loc[201103:201506].as_matrix(), cplags, 1)\n",
    "columns = ['tb0', 'tb1', 'mr0', 'mr1', 'cp0', 'cp1']\n",
    "z = pd.DataFrame( np.concatenate( (ztb, zmr, zcp), axis=1), y.index.values, columns )\n",
    "X = sm.add_constant( z )\n",
    "\n",
    "# Fit macro model\n",
    "eq = sm.OLS(y, X)\n",
    "fit = eq.fit()\n",
    "\n",
    "# Predict with macro model\n",
    "test_cpi = df.cpi.loc[201507:201605]\n",
    "test_index = test_cpi.index\n",
    "ztb_test = almonZmatrix(df.balance_trade.loc[201502:201605].as_matrix(), tblags, 1)\n",
    "zmr_test = almonZmatrix(df.mortgage_rate.loc[201502:201605].as_matrix(), mrlags, 1)\n",
    "zcp_test = almonZmatrix(lncpi.loc[201502:201605].as_matrix(), cplags, 1)\n",
    "z_test = pd.DataFrame( np.concatenate( (ztb_test, zmr_test, zcp_test), axis=1), \n",
    "                       test_index, columns )\n",
    "X_test = sm.add_constant( z_test )\n",
    "pred_lnrp = fit.predict( X_test )\n",
    "pred_p = np.exp(pred_lnrp) * test_cpi\n",
    "\n",
    "# Merge with test cases and compute mean for macro prediction\n",
    "test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n",
    "test[\"year\"]  = test[\"timestamp\"].dt.year\n",
    "test[\"month\"] = test[\"timestamp\"].dt.month\n",
    "test[\"yearmonth\"] = 100*test.year + test.month\n",
    "test_ids = test[[\"yearmonth\",\"id\"]]\n",
    "monthprices = pd.DataFrame({\"yearmonth\":pred_p.index.values,\"monthprice\":pred_p.values})\n",
    "macro_mean = np.exp(test_ids.merge(monthprices, on=\"yearmonth\").monthprice.apply(np.log).mean())\n",
    "macro_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7773440.7512487005"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive macro model assumes housing prices will simply follow CPI\n",
    "naive_pred_lnrp = y.mean()\n",
    "naive_pred_p = np.exp(naive_pred_lnrp) * test_cpi\n",
    "monthnaive = pd.DataFrame({\"yearmonth\":pred_p.index.values, \"monthprice\":naive_pred_p.values})\n",
    "macro_naive = np.exp(test_ids.merge(monthnaive, on=\"yearmonth\").monthprice.apply(np.log).mean())\n",
    "macro_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6637341.6089008758"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine naive and substantive macro models\n",
    "macro_mean = macro_naive * (macro_mean/macro_naive) ** macro_humility_factor\n",
    "macro_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fix: ', 550)\n",
      "('Fix: ', 149)\n"
     ]
    }
   ],
   "source": [
    "filenames = [\"./data/macro.csv\",\"./data/train.csv/train.csv\",\"./data/test.csv/test.csv\"]\n",
    "[macro, train, test] = read_data(filenames)\n",
    "train, test = map(bad_address, [train, test])\n",
    "train=er10(er10(train))\n",
    "train, test = map(my_transport, [train, test])\n",
    "train, test = map(my_cafe, [train, test])\n",
    "id_test = test.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    2662\n",
       "1.0    2266\n",
       "3.0    1913\n",
       "4.0     127\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean data\n",
    "equal_index = [601,1896,2791]\n",
    "test.ix[equal_index, \"life_sq\"] = test.ix[equal_index, \"full_sq\"]\n",
    "\n",
    "kitch_is_build_year = [13117]\n",
    "train.ix[kitch_is_build_year, \"build_year\"] = train.ix[kitch_is_build_year, \"kitch_sq\"]\n",
    "\n",
    "bad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\n",
    "train.ix[bad_index, \"num_room\"] = np.NaN\n",
    "\n",
    "bad_index = [3174, 7313]\n",
    "test.ix[bad_index, \"num_room\"] = np.NaN\n",
    "\n",
    "[train, test] = [clean_sq(train), clean_sq(test)]\n",
    "\n",
    "train.product_type.value_counts(normalize= True)\n",
    "test.product_type.value_counts(normalize= True)\n",
    "\n",
    "\n",
    "train.floor.describe(percentiles= [0.9999])\n",
    "bad_index = [23584]\n",
    "train.ix[bad_index, \"floor\"] = np.NaN\n",
    "train.material.value_counts()\n",
    "test.material.value_counts()\n",
    "train.state.value_counts()\n",
    "bad_index = train[train.state == 33].index\n",
    "train.ix[bad_index, \"state\"] = np.NaN\n",
    "test.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# brings error down a lot by removing extreme price per sqm\n",
    "train.loc[train.full_sq == 0, 'full_sq'] = 50\n",
    "train = train[train.price_doc/train.full_sq <= 600000]\n",
    "train = train[train.price_doc/train.full_sq >= 10000]\n",
    "#mult = .969 train[\"price_doc\"] * mult + 10\n",
    "train[\"persqm\"] = train[\"price_doc\"]/train.full_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[train, test] = [add_dates(train), add_dates(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Other feature engineering\n",
    "train, test = my_impute(train), my_impute(test)\n",
    "train, test = new_features(train), new_features(test)\n",
    "v=['hospital_beds_raion']\n",
    "train, test = train.drop(v, axis=1), test.drop(v, axis=1)\n",
    "test.full_sq = test.full_sq.fillna(1.5*test.life_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = train[\"price_doc\"]\n",
    "x_train = train.drop([\"id\", \"timestamp\", \"price_doc\", \"persqm\"], axis=1)\n",
    "x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n",
    "\n",
    "x_train, x_test = labels(x_train), labels(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v=printna(x_train).index\n",
    "#x_train, x_test = x_train.drop(v, axis=1), x_test.drop(v, axis=1)\n",
    "x_train, x_test = x_train.fillna(0), x_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=np.array(x_train)\n",
    "Y=np.array(y_train)\n",
    "X_test=np.array(x_test)\n",
    "Norm = MinMaxScaler()\n",
    "Xt =Norm.fit_transform(X)\n",
    "Xt_test=Norm.transform(X_test)\n",
    "Yt = (Y*1.0-min(Y))/(max(Y)-min(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def larger_model(dim, weight_reg=0.01, activ_reg=0.01, dp=0.2):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim, input_dim=dim, kernel_initializer='random_normal', activation='relu', \n",
    "                    kernel_regularizer=regularizers.l2(weight_reg),activity_regularizer=regularizers.l1(activ_reg)))\n",
    "    model.add(Dropout(dp))\n",
    "    model.add(Dense(148, kernel_initializer='random_normal', activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(weight_reg),activity_regularizer=regularizers.l1(activ_reg)))\n",
    "    model.add(Dropout(dp))\n",
    "    model.add(Dense(20, kernel_initializer='random_normal', activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(weight_reg),activity_regularizer=regularizers.l1(activ_reg)))\n",
    "    model.add(Dropout(dp))\n",
    "    model.add(Dense(1, kernel_initializer='random_normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN = larger_model(dim=Xt.shape[1], weight_reg=0.01, activ_reg=0.01, dp=0.2)\n",
    "#NN = load_model('models/drop274r10-0609.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30403/30403 [==============================] - 0s - loss: 15628479954105.4805     \n",
      "Epoch 2/5\n",
      "30403/30403 [==============================] - 0s - loss: 15769416231546.3457     \n",
      "Epoch 3/5\n",
      "30403/30403 [==============================] - 0s - loss: 15446850288140.5449     \n",
      "Epoch 4/5\n",
      "30403/30403 [==============================] - 0s - loss: 15711142242911.4004     \n",
      "Epoch 5/5\n",
      "30403/30403 [==============================] - 0s - loss: 16089292054483.0020     \n",
      "60 -0.890849788463\n"
     ]
    }
   ],
   "source": [
    "seq=[60]\n",
    "for i in seq:\n",
    "    NN.fit(X,Y, batch_size=X.shape[0]/i, epochs=300, verbose=0)\n",
    "    NN.fit(X,Y, batch_size=X.shape[0]/i, epochs=5, verbose=1)\n",
    "    print i, r2_score(NN.predict(X),Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 6731331. ],\n",
       "        [ 7526640.5],\n",
       "        [ 6251198. ],\n",
       "        ..., \n",
       "        [ 5731664. ],\n",
       "        [ 6631263. ],\n",
       "        [ 5703804.5]], dtype=float32),\n",
       " array([ 5850000,  6000000,  5700000, ...,  6970959, 13500000,  5600000]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.predict(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.5950832562\n"
     ]
    }
   ],
   "source": [
    "print r2_score(NN.predict(Xt),Yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN.save('models/drop274r10-0609.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Yt_test = NN.predict(Xt_test)\n",
    "test['price_doc'] = (max(Y)-min(Y))*Yt_test + min(Y)\n",
    "#test[\"price_doc\"] =test['persqm']*test[\"full_sq\"]\n",
    "#out = test[[\"id\",\"price_doc\"]]\n",
    "#out.to_csv(\"./nn2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5.404131e+07\n",
       "1       8.171049e+06\n",
       "2      -4.924844e+05\n",
       "3       6.487564e+07\n",
       "4       2.382602e+06\n",
       "5       2.264605e+06\n",
       "6       1.608603e+06\n",
       "7       1.845307e+06\n",
       "8       9.914066e+06\n",
       "9       1.040796e+08\n",
       "10     -7.071814e+05\n",
       "11      2.178462e+06\n",
       "12      2.141564e+06\n",
       "13      8.625219e+05\n",
       "14      3.829185e+08\n",
       "15      3.430434e+06\n",
       "16      1.514366e+06\n",
       "17      2.136032e+06\n",
       "18      9.459892e+05\n",
       "19      9.525724e+06\n",
       "20      2.119675e+06\n",
       "21      7.270033e+05\n",
       "22      8.490811e+05\n",
       "23      1.606674e+06\n",
       "24     -2.760079e+06\n",
       "25     -6.631009e+05\n",
       "26      2.644038e+05\n",
       "27     -1.647346e+05\n",
       "28      1.337572e+06\n",
       "29      1.649194e+06\n",
       "            ...     \n",
       "7632   -4.221885e+05\n",
       "7633    7.391184e+05\n",
       "7634    2.349940e+07\n",
       "7635    1.377595e+06\n",
       "7636    2.859038e+08\n",
       "7637    1.478190e+08\n",
       "7638    2.641069e+08\n",
       "7639   -5.094874e+05\n",
       "7640    2.265302e+07\n",
       "7641    2.705324e+08\n",
       "7642    1.053888e+06\n",
       "7643    3.511938e+06\n",
       "7644    3.348321e+05\n",
       "7645    2.325257e+08\n",
       "7646   -4.015542e+04\n",
       "7647    3.852036e+06\n",
       "7648    8.566405e+08\n",
       "7649    6.117122e+07\n",
       "7650    2.137614e+06\n",
       "7651    3.084420e+06\n",
       "7652    3.692482e+06\n",
       "7653    8.836969e+08\n",
       "7654    1.815318e+08\n",
       "7655    9.985809e+05\n",
       "7656   -4.282648e+04\n",
       "7657    5.034691e+07\n",
       "7658    1.296119e+08\n",
       "7659    3.254510e+07\n",
       "7660    1.112030e+05\n",
       "7661   -8.062844e+03\n",
       "Name: persqm, dtype: float32"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['persqm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# evaluate model with standardized dataset\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=50, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=2, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='r2')\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
